{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf3af3df",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f59aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.inputSweep import config\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from scipy.stats import median_abs_deviation, chi2\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from tqdm.notebook import tqdm\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.tsa.stattools import adfuller, acf, q_stat\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from statsmodels.nonparametric.kde import KDEUnivariate\n",
    "from scipy.stats import friedmanchisquare\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bc554f",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2089d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "expSets = [config.experiments[i] for i in range(1,35)]\n",
    "\n",
    "save_path = ...\n",
    "modulation = \"frequency\" # \"frequency\"\n",
    "if modulation == \"amplitude\":\n",
    "    Settings = [\"amplitude\"]\n",
    "    n_samples = 81\n",
    "    n_tries = 110\n",
    "elif modulation == \"frequency\":\n",
    "    Settings = [\"frequency\"]\n",
    "    n_samples = 49\n",
    "    n_tries = 55\n",
    "expSetsAmp = []\n",
    "for expSet in expSets:\n",
    "    for Setting in Settings:\n",
    "        if Setting in expSet[4]:\n",
    "            expSetAmp = deepcopy(expSet)\n",
    "            expSetAmp[4] = [Setting]\n",
    "            expSetsAmp.append(expSetAmp)\n",
    "\n",
    "n_subsets = 9\n",
    "names = [\n",
    "    \"2_0_\",\n",
    "    \"2_1_\",\n",
    "    \"3_0_\",\n",
    "    \"3_1_\",\n",
    "    \"3_2_\",\n",
    "    \"4_0_\",\n",
    "    \"4_1_\",\n",
    "    \"4_2_\",\n",
    "    \"4_3_\",\n",
    "]\n",
    "            \n",
    "window_list = np.arange(20, 201, 20)\n",
    "number_of_bins = 8\n",
    "totalNrNetworks = sum([len(e[0]) for e in expSetsAmp]) \n",
    "capacities = np.zeros((totalNrNetworks,4,len(window_list),number_of_bins))\n",
    "capacities_b_small = np.zeros((totalNrNetworks,4,len(window_list),number_of_bins))\n",
    "probabilities = np.zeros((totalNrNetworks,4,len(window_list),number_of_bins,n_samples))\n",
    "encoding_values = np.zeros((totalNrNetworks,4,len(window_list),n_samples,n_tries)) \n",
    "n_bins = 250 * (np.arange(1, 7+1))\n",
    "probabilities_y = np.zeros((totalNrNetworks,4,len(window_list),n_samples,250)) \n",
    "capacities_est = np.zeros((totalNrNetworks,4,n_subsets,number_of_bins)) \n",
    "capacities_est_b_small = np.zeros((totalNrNetworks,4,n_subsets,number_of_bins)) \n",
    "probability_y_est = np.zeros((totalNrNetworks,4,n_subsets,n_samples,250)) \n",
    "probability_est = np.zeros((totalNrNetworks,4,n_subsets,number_of_bins,n_samples)) \n",
    "p_values = np.zeros((totalNrNetworks,4,len(window_list),n_samples)) \n",
    "tot_len = 0\n",
    "for j, expSet in enumerate(expSetsAmp): \n",
    "    for m,network in enumerate(expSet[0]): \n",
    "        filepath = os.path.join(save_path, f\"processedAdv/{expSet[1]}_{expSet[4][0]}_{network}{expSet[-2]}.h5\") \n",
    "        with h5py.File(filepath, 'r') as h5file: \n",
    "            print(filepath)\n",
    "            for i, w in enumerate(window_list): \n",
    "                probabilities[tot_len+m,0,i,] = h5file[f\"Capacity_Estimation/Rate/w{w}_p\"][:]\n",
    "                probabilities[tot_len+m,3,i,] = h5file[f\"Capacity_Estimation/Latency/w{w}_p\"][:]\n",
    "                probabilities[tot_len+m,1,i,] = h5file[f\"Capacity_Estimation/ISI/w{w}_p\"][:]\n",
    "                probabilities[tot_len+m,2, i,] = h5file[f\"Capacity_Estimation/Phase/w{w}_p\"][:]\n",
    "                probabilities_y[tot_len+m,0,i,] = h5file[f\"Capacity_Estimation/Rate/w{w}p_y\"][:]\n",
    "                probabilities_y[tot_len+m,3,i,] = h5file[f\"Capacity_Estimation/Latency/w{w}p_y\"][:]\n",
    "                probabilities_y[tot_len+m,1,i,] = h5file[f\"Capacity_Estimation/ISI/w{w}p_y\"][:]\n",
    "                probabilities_y[tot_len+m,2, i,] = h5file[f\"Capacity_Estimation/Phase/w{w}p_y\"][:]\n",
    "                p_values[tot_len+m,0,i,] = h5file[f\"AIS/Rate/w{w}_p_ais\"][:]\n",
    "                p_values[tot_len+m,3,i,] = h5file[f\"AIS/Latency/w{w}_p_ais\"][:]\n",
    "                p_values[tot_len+m,1,i,] = h5file[f\"AIS/ISI/w{w}_p_ais\"][:]\n",
    "                p_values[tot_len+m,2, i,] = h5file[f\"AIS/Phase/w{w}_p_ais\"][:]\n",
    "                encoding_values[tot_len+m,0,i,] = np.mean(h5file[f\"Rate/w{w}\"][:],axis=-1)\n",
    "                encoding_values[tot_len+m,3,i,] = np.mean(h5file[f\"Latency/w{w}\"][:],axis=-1)\n",
    "                encoding_values[tot_len+m,1,i,] = np.mean(h5file[f\"ISI/w{w}\"][:],axis=-1)\n",
    "                encoding_values[tot_len+m,2, i,] = np.mean(h5file[f\"Phase/w{w}\"][:],axis=-1)\n",
    "                capacities[tot_len+m,0,i,] = h5file[f\"Capacity_Estimation/Rate/w{w}_c\"][:]\n",
    "                capacities[tot_len+m,3,i,] = h5file[f\"Capacity_Estimation/Latency/w{w}_c\"][:]\n",
    "                capacities[tot_len+m,1,i,] = h5file[f\"Capacity_Estimation/ISI/w{w}_c\"][:]\n",
    "                capacities[tot_len+m,2, i,] = h5file[f\"Capacity_Estimation/Phase/w{w}_c\"][:]\n",
    "            for i in range(n_subsets):\n",
    "                probability_est[tot_len+m,0,i,] = h5file[f\"Capacity_Estimation_Small/Rate/{names[i]}p\"][:]\n",
    "                probability_est[tot_len+m,3,i,] = h5file[f\"Capacity_Estimation_Small/Latency/{names[i]}p\"][:]\n",
    "                probability_est[tot_len+m,1,i,] = h5file[f\"Capacity_Estimation_Small/ISI/{names[i]}p\"][:]\n",
    "                probability_est[tot_len+m,2,i,] = h5file[f\"Capacity_Estimation_Small/Phase/{names[i]}p\"][:]\n",
    "                probability_y_est[tot_len+m,0,i,] = h5file[f\"Capacity_Estimation_Small/Rate/{names[i]}p_y\"][:]\n",
    "                probability_y_est[tot_len+m,3,i,] = h5file[f\"Capacity_Estimation_Small/Latency/{names[i]}p_y\"][:]\n",
    "                probability_y_est[tot_len+m,1,i,] = h5file[f\"Capacity_Estimation_Small/ISI/{names[i]}p_y\"][:]\n",
    "                probability_y_est[tot_len+m,2,i,] = h5file[f\"Capacity_Estimation_Small/Phase/{names[i]}p_y\"][:]\n",
    "                capacities_est[tot_len+m,0,i,] = h5file[f\"Capacity_Estimation_Small/Rate/{names[i]}c\"][:]\n",
    "                capacities_est[tot_len+m,3,i,] = h5file[f\"Capacity_Estimation_Small/Latency/{names[i]}c\"][:]\n",
    "                capacities_est[tot_len+m,1,i,] = h5file[f\"Capacity_Estimation_Small/ISI/{names[i]}c\"][:]\n",
    "                capacities_est[tot_len+m,2,i,] = h5file[f\"Capacity_Estimation_Small/Phase/{names[i]}c\"][:]\n",
    "                capacities_est_b_small[tot_len+m,0,i,] = h5file[f\"Capacity_Estimation_Small/Rate/{names[i]}c\"][:]\n",
    "                capacities_est_b_small[tot_len+m,3,i,] = h5file[f\"Capacity_Estimation_Small/Latency/{names[i]}c\"][:]\n",
    "                capacities_est_b_small[tot_len+m,1,i,] = h5file[f\"Capacity_Estimation_Small/ISI/{names[i]}c\"][:]\n",
    "                capacities_est_b_small[tot_len+m,2,i,] = h5file[f\"Capacity_Estimation_Small/Phase/{names[i]}c\"][:]\n",
    "    tot_len += len(expSet[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4498606a",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248e6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_to_use = 6#0-7\n",
    "# Step 1: Filter out trials where no capacity value exceeds 1\n",
    "capacity_looked_at = capacities\n",
    "valid_trials = np.any(capacity_looked_at[...,bin_to_use] > 1, axis=(1, 2))  # Check across all capacity types and windows\n",
    "y_lim = (-0.25,2.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefc6182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter capacities to include only valid trials\n",
    "filtered_capacities = capacity_looked_at[valid_trials,...,bin_to_use] # -capacities_noise\n",
    "#filtered_capacities = np.max(capacities[valid_trials],axis=-1) # -capacities_noise\n",
    "#filtered_capacities -= np.mean(filtered_capacities,axis=(1,2))[:,None,None]\n",
    "# Calculate the mean and standard deviation over the first dimension\n",
    "mean_capacities = np.mean(filtered_capacities, axis=0)\n",
    "#Standard Error of the Mean\n",
    "std_capacities = np.std(filtered_capacities, axis=0) / np.sqrt(filtered_capacities.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3941518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert window_list to milliseconds\n",
    "window_list = np.arange(20, 201, 20)\n",
    "window_ms = np.array(window_list) / 20\n",
    "# Colors and labels for the four capacity types (use colorblind-friendly palette)\n",
    "colors = ['#17becf', '#d95f02', '#7570b3', '#e7298a']\n",
    "styles = ['solid','dotted','dashed','dashdot']\n",
    "labels = ['Rate', 'ISI', 'Phase', 'TTFS']\n",
    "symbols = ['D','s','o','v']\n",
    "shifts = [-0.15,-0.05,0.05,0.15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f2c00f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set up the plot with standard figure size for a journal\n",
    "fig, ax = plt.subplots(figsize=(8, 6))  # Adjust size to be more standard for a paper\n",
    "\n",
    "# Set axis background to white (default)\n",
    "ax.set_facecolor('white')\n",
    "for i in range(4):\n",
    "    # Line connecting the mean values\n",
    "    ax.plot(window_ms+shifts[i], mean_capacities[i, :], label=labels[i], color=colors[i], linewidth=4)\n",
    "    #for j,c in enumerate(filtered_capacities):\n",
    "    #    ax.plot(window_ms, c[i, :], alpha=0.7, color=colors[j], linewidth=2, linestyle=styles[i])\n",
    "for i in range(4):    \n",
    "    # Error bars representing standard deviation at each point\n",
    "    ax.scatter(window_ms+shifts[i], mean_capacities[i, :],marker=symbols[i], s=100, color=colors[i])\n",
    "    ax.errorbar(window_ms+shifts[i], mean_capacities[i, :], yerr=std_capacities[i, :], \n",
    "                color=colors[i], capsize=3)\n",
    "\n",
    "# Axis labels with appropriate fonts for a journal\n",
    "plt.xlabel(\"Window Size [ms]\", fontsize=14)\n",
    "plt.ylabel(\"Capacity [bits / channel use]\", fontsize=14)\n",
    "plt.ylim(y_lim)\n",
    "plt.xlim(0.5,10.5)\n",
    "plt.legend(fontsize=12, loc='lower right')\n",
    "# Ticks with standard font sizes\n",
    "plt.tick_params(axis='both', labelsize=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be34a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the new plot for each network's last entry values\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Loop over the 4 capacity types and plot their values across networks\n",
    "sorted_indices = np.argsort(filtered_capacities[:,0,-1])\n",
    "for i in range(4):\n",
    "    # Scatter plot for the i-th capacity type\n",
    "    ax.scatter(np.arange(len(filtered_capacities))+1, filtered_capacities[sorted_indices, i, -1], label=labels[i], \n",
    "               color=colors[i], marker=symbols[i], s=100)\n",
    "\n",
    "# Axis labels\n",
    "plt.xlabel(\"Network\", fontsize=14)\n",
    "plt.ylabel(\"Capacity [bits / channel use]\", fontsize=14)\n",
    "plt.ylim(y_lim)\n",
    "# Ticks with standard font sizes\n",
    "plt.tick_params(axis='both', labelsize=15)\n",
    "ax.set_xticks(np.arange(len(filtered_capacities))+1)\n",
    "# Adding a legend with standard font size\n",
    "plt.legend(fontsize=12, loc='lower right')\n",
    "ax.set_xticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d88b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the new plot for each network's last entry values\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Loop over the 4 capacity types and plot their values across networks\n",
    "#for i in range(4):\n",
    "#    # Scatter plot for the i-th capacity type\n",
    "#    ax.scatter(np.arange(len(filtered_capacities))+1, filtered_capacities[:, i, -1], label=labels[i], \n",
    "#               color=colors[i], marker=symbols[i], s=100)\n",
    "\n",
    "cdict = {\n",
    "    'red':   [(0.0, 1.0, 1.0),  # Yellow\n",
    "              (0.5, 0.5, 0.5),  # Transition to purple\n",
    "              (1.0, 0.5, 0.5)], # Purple\n",
    "    'green': [(0.0, 1.0, 1.0),\n",
    "              (0.5, 0.0, 0.0),\n",
    "              (1.0, 0.0, 0.0)],\n",
    "    'blue':  [(0.0, 0.0, 0.0),\n",
    "              (0.5, 0.5, 0.5),\n",
    "              (1.0, 0.5, 0.5)]\n",
    "}\n",
    "\n",
    "#cmap = LinearSegmentedColormap('YellowPurple', segmentdata=cdict, N=256)\n",
    "cmap = plt.get_cmap('summer')\n",
    "sorted_indices = np.argsort(filtered_capacities[:,0,-1])[::-1]\n",
    "noise = np.random.random(len(sorted_indices))*0.3-0.15\n",
    "for j, index in enumerate(sorted_indices):\n",
    "    # Scatter plot for the i-th capacity type# Loop over each network index\n",
    "    plt.plot(np.arange(4)+noise[index],filtered_capacities[index, :, -1],c=cmap(j/len(filtered_capacities)),zorder=1)\n",
    "for j, index in enumerate(sorted_indices):\n",
    "    for i in range(4):\n",
    "        ax.scatter(i+noise[index], filtered_capacities[index, i, -1], \n",
    "                   color=colors[i], marker=symbols[i], s=100,zorder=2)\n",
    "\n",
    "# Axis labels\n",
    "#plt.xlabel(\"Network Index\", fontsize=14)\n",
    "plt.ylabel(\"Capacity [bits / channel use]\", fontsize=14)\n",
    "plt.ylim(y_lim)\n",
    "plt.xlim(-0.5,3.5)\n",
    "# Ticks with standard font sizes # Adjust 0.15 based on your offset calculation\n",
    "# Set x-axis ticks at these positions\n",
    "# Axis labels with appropriate fonts for a journal\n",
    "plt.xlabel(\"Encoding\", fontsize=14)\n",
    "\n",
    "# Ticks with standard font sizes\n",
    "plt.tick_params(axis='both', labelsize=15)\n",
    "ax.set_xticks(range(4))\n",
    "ax.set_xticklabels(labels, fontsize=15)\n",
    "# Adding a legend with standard font size\n",
    "#plt.legend(fontsize=12, loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beddb719",
   "metadata": {},
   "outputs": [],
   "source": [
    "w=-1\n",
    "valid_trials = np.any(capacities[:,:,w] > 1, axis=(1, 2))\n",
    "capacity_corrected = capacities[valid_trials]# - capacities_noise[valid_trials]\n",
    "for k in range(capacity_corrected.shape[1]):\n",
    "    # Calculate mean and standard error for error bars\n",
    "    mean_values = np.mean(capacity_corrected[:, k, w], axis=0)\n",
    "    error_values = np.std(capacity_corrected[:, k, w], axis=0) / np.sqrt(capacity_corrected.shape[0])\n",
    "    \n",
    "    # Plot with error bars\n",
    "    plt.errorbar(\n",
    "        x=np.arange(len(mean_values))+1,  # Assuming the x-axis is indices\n",
    "        y=mean_values,\n",
    "        yerr=error_values,\n",
    "        label=labels[k],\n",
    "        color=colors[k],\n",
    "        linewidth=2,\n",
    "        capsize=5  # Add caps to the error bars\n",
    "    )\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"X-axis Label\")  # Customize as needed\n",
    "plt.ylabel(\"Y-axis Label\")  # Customize as needed\n",
    "plt.title(\"Plot with Error Bars\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d5ea62",
   "metadata": {},
   "source": [
    "# AIS Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280b65ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Combine within Durations for Each Experiment and Sample\n",
    "for k in range(4): \n",
    "    for w in range(10):\n",
    "        data = p_values[:, k, w, :]\n",
    "        weights = np.zeros_like(data)+1\n",
    "        weights[encoding_values[:,0, w].mean(axis=-1)>0.1] = 10#weights + np.exp()\n",
    "        #for sample in range(n_biological_samples):\n",
    "        #    print(combine_pvalues(np.clip(data[sample].flatten(), 1e-300, 1-1e-15),method=\"stouffer\",weights=weights[sample].flatten())[1])\n",
    "            #print(np.count_nonzero(data[sample]<0.05)/len(data[sample].flatten()),\"%\")\n",
    "        # Step 3: Combine Across All Biological Samples for Global P-Value\n",
    "        global_combined_p = combine_pvalues(np.clip(data.flatten(), 1e-300, 1-1e-15),method=\"stouffer\",weights=weights.flatten())[1]\n",
    "        #print(np.count_nonzero(data<0.05)/len(data.flatten()),\"%\")\n",
    "        print(\"Global Combined p-value:\", global_combined_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390af3ff",
   "metadata": {},
   "source": [
    "# ADF Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f8a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Combine within Durations for Each Experiment and Sample\n",
    "combined_p_stouffer = np.zeros(4)\n",
    "combined_p_fisher = np.zeros(4)\n",
    "n_biological_samples = encoding_values.shape[0]\n",
    "n_time_windows = encoding_values.shape[2]\n",
    "n_experiments = encoding_values.shape[3]\n",
    "adf_pvals = np.empty((4,n_biological_samples, n_time_windows,n_experiments))\n",
    "pbar = tqdm(total=4*n_biological_samples*n_time_windows*n_experiments)\n",
    "for k in range(4): \n",
    "    data = encoding_values[:, k, :, :]\n",
    "    for sample_idx in range(n_biological_samples):\n",
    "        for window_idx in range(n_time_windows):\n",
    "            for exp_idx in range(n_experiments):\n",
    "                if not data[sample_idx, window_idx, exp_idx].min()==data[sample_idx, window_idx, exp_idx].max():\n",
    "                    series = data[sample_idx, window_idx, exp_idx]\n",
    "                    _, p_value, _, _, _, _ = adfuller(series)\n",
    "                    if not np.isnan(p_value):\n",
    "                        adf_pvals[k,sample_idx, window_idx,exp_idx] =  p_value\n",
    "                pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ca2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Combine within Durations for Each Experiment and Sample\n",
    "for k in range(4): \n",
    "    for w in range(10):\n",
    "        data = adf_pvals[k, :, w, :]\n",
    "        weights = np.zeros_like(data)+1\n",
    "        weights[encoding_values[:,0, w].mean(axis=-1)>0.1] = 10#weights + np.exp()\n",
    "        global_combined_p = combine_pvalues(np.clip(data.flatten(), 1e-300, 1-1e-15),method=\"stouffer\",weights=weights.flatten())[1]\n",
    "        print(\"Global Combined p-value:\", global_combined_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d132e980",
   "metadata": {},
   "source": [
    "# Wilcoxon Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f27e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "from itertools import combinations\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "# Simulate your data (replace with your actual array)\n",
    "# Shape: (n_exp, 4, n_time)\n",
    "\n",
    "n_exp, n_encodings, n_time = filtered_capacities.shape\n",
    "\n",
    "# Preallocate matrices for results\n",
    "pairwise_comparisons = list(combinations(range(n_encodings), 2))\n",
    "significance_matrix = np.zeros((len(pairwise_comparisons), n_time), dtype=int)\n",
    "p_value_matrix = np.full((len(pairwise_comparisons), n_time), np.nan)\n",
    "\n",
    "for t in range(n_time):\n",
    "    time_data = filtered_capacities[:, :, t]  # Data for all encodings at time t\n",
    "    pairwise_pvals = []\n",
    "    encoding_pairs = list(combinations(range(n_encodings), 2))  # All encoding pairs\n",
    "    \n",
    "    # Perform pairwise Wilcoxon signed-rank tests\n",
    "    for enc1, enc2 in encoding_pairs:\n",
    "        try:\n",
    "            _, p_value = wilcoxon(time_data[:, enc1], time_data[:, enc2])\n",
    "        except ValueError:\n",
    "            p_value = np.nan  # Handle edge cases with insufficient variation\n",
    "        pairwise_pvals.append(p_value)\n",
    "    \n",
    "    # Correct for multiple comparisons (FDR correction)\n",
    "    corrected_pvals = multipletests(pairwise_pvals, method='fdr_bh')[1]\n",
    "    \n",
    "    # Populate matrices\n",
    "    for i, ((enc1, enc2), corr_p_val) in enumerate(zip(encoding_pairs, corrected_pvals)):\n",
    "        p_value_matrix[i, t] = corr_p_val\n",
    "        significance_matrix[i, t] = int(corr_p_val < 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b9f44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert window_list to milliseconds\n",
    "time_labels = [f\"{int(w / 20)} ms\" for w in window_list]\n",
    "\n",
    "# Construct LaTeX table header\n",
    "header = (\n",
    "    r\"\\begin{table}[ht]\" + \"\\n\"\n",
    "    + r\"\\centering\" + \"\\n\"\n",
    "    + r\"\\small\" + \"\\n\"  # Set table text to small size\n",
    "    + r\"\\begin{tabular}{l\" + \"c\" * n_time + r\"}\" + \"\\n\"\n",
    "    + r\"\\hline\" + \"\\n\"\n",
    "    + \"Pairwise Comparison\"\n",
    "    + \"\".join([f\" & {t}\" for t in time_labels])\n",
    "    + r\" \\\\\" + \"\\n\"\n",
    "    + r\"\\hline\" + \"\\n\"\n",
    ")\n",
    "\n",
    "# Generate LaTeX table rows\n",
    "latex_rows = []\n",
    "for i, (enc1, enc2) in enumerate(pairwise_comparisons):\n",
    "    row = f\"{labels[enc1]} vs {labels[enc2]}\"\n",
    "    for t in range(n_time):\n",
    "        # Bold significant values, round to 3 decimal places\n",
    "        p_val = f\"{p_value_matrix[i][t]:.3f}\"\n",
    "        if significance_matrix[i][t] == 1:\n",
    "            row += f\" & \\\\textbf{{{p_val}}}\"\n",
    "        else:\n",
    "            row += f\" & {p_val}\"\n",
    "    row += r\" \\\\\"  # Properly escape LaTeX newline\n",
    "    latex_rows.append(row)\n",
    "\n",
    "# Combine rows into the body\n",
    "body = \"\\n\".join(latex_rows) + \"\\n\" + r\"\\hline\" + \"\\n\"\n",
    "\n",
    "# Footer of the LaTeX table\n",
    "footer = (\n",
    "    r\"\\end{tabular}\" + \"\\n\"\n",
    "    + r\"\\caption{Pairwise comparisons of encodings across time points. Significant p-values are bolded.}\" + \"\\n\"\n",
    "    + r\"\\label{tab:pairwise_comparisons}\" + \"\\n\"\n",
    "    + r\"\\end{table}\"\n",
    ")\n",
    "\n",
    "# Combine all parts\n",
    "latex_table = header + body + footer\n",
    "\n",
    "# Output properly formatted LaTeX\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"```latex\\n{latex_table}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0fa3f8",
   "metadata": {},
   "source": [
    "# Capacity Estimation Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278684b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_trials = np.any(capacities_est > 1, axis=(1, 2, 3))  # Check across all capacity types and windows\n",
    "capacities_est_corrected = capacities_est[valid_trials]#-capacities_est_noise\n",
    "#capacities_est_corrected = (capacities_est)#[valid_trials]#-capacities_est_noise\n",
    "capacities_1 = capacities[valid_trials,:,-1]\n",
    "capacities_2 = np.mean(capacities_est_corrected[:,:,[0,1]],axis=-2)\n",
    "capacities_3 = np.mean(capacities_est_corrected[:,:,[2,3,4]],axis=-2)\n",
    "capacities_4 = np.mean(capacities_est_corrected[:,:,[5,6,7,8]],axis=-2)\n",
    "capacities_check = np.stack((capacities_1,capacities_2,capacities_3,capacities_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3eff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(capacities_check.shape[2]):\n",
    "    data = capacities_check[:,:,k,bin_to_use]\n",
    "    #data = np.diff(data,axis=0)\n",
    "    plt.plot(np.arange(4),np.mean(data,axis=1),colors[k],linewidth=5)\n",
    "    plt.plot(np.arange(4),data,colors[k],linestyle=\"dotted\")\n",
    "plt.title(f\"{2**(bin_to_use+1)}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.abs(np.mean(np.diff(capacities_check,axis=0),axis=0))\n",
    "data_mean = np.mean(data,axis=0)\n",
    "data_std = np.std(data,axis=0)\n",
    "for k in range(capacities_check.shape[2]):\n",
    "    #plt.plot(np.arange(1,data_mean[k].shape[-1])+1,data[:,k,1:].T,color=colors[k])\n",
    "    plt.errorbar(x=np.arange(1,data_mean[k].shape[-1])+1+shifts[k], y=data_mean[k,1:],yerr=data_std[k,1:],label=labels[k],color=colors[k],linewidth=2,capsize=5)\n",
    "    #plt.legend()\n",
    "    #plt.plot(np.arange(3),data,colors[k],linestyle=\"dotted\")\n",
    "    #plt.title(f\"{2**(b+1)}\")\n",
    "plt.errorbar(x=np.arange(1,data_mean[k].shape[-1])+1, y=np.mean(data_mean[:,1:],axis=0),yerr=np.mean(data_std[:,1:],axis=0),label=\"Mean\",color=\"black\",linewidth=2,capsize=5)\n",
    "plt.ylim(0,0.14)\n",
    "# Axis labels with appropriate fonts for a journal\n",
    "plt.xticks(np.arange(1,data_mean[k].shape[-1])+1, [f\"$2^{{{int(b)}}}$\" for b in np.arange(1,data_mean[k].shape[-1])+1])\n",
    "plt.xlabel(\"# Bins\", fontsize=12)\n",
    "plt.ylabel(\"Absolute Difference [Bits / Channel Use]\", fontsize=12)\n",
    "# Ticks with standard font sizes\n",
    "plt.tick_params(axis='both', labelsize=13)\n",
    "#plt.legend()\n",
    "#plt.savefig(f\"/itet-stor/kjoel/neuronies/biohybrid_cmos/zzz_Joel_copycat_pfft_howlong_can_we_make_folder_names/bias{Settings}.pdf\", transparent=True, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
